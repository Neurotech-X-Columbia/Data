{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k0w3lDbOj0D",
        "outputId": "30164345-5982-4998-f6d9-07b3bdbefb88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data'...\n",
            "remote: Enumerating objects: 4179, done.\u001b[K\n",
            "remote: Counting objects: 100% (281/281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (234/234), done.\u001b[K\n",
            "remote: Total 4179 (delta 45), reused 278 (delta 44), pack-reused 3898\u001b[K\n",
            "Receiving objects: 100% (4179/4179), 240.36 MiB | 13.76 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n",
            "Checking out files: 100% (3992/3992), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Initalize Script (may take a few minutes)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from matplotlib.mlab import psd\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import tabulate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "from pywt import wavedec\n",
        "from functools import reduce\n",
        "from scipy import signal\n",
        "from scipy.stats import entropy\n",
        "from scipy.fft import fft, ifft\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold,cross_validate\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "import matplotlib.pyplot as plt;\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D,Conv2D,Add\n",
        "from tensorflow.keras.layers import MaxPool1D, MaxPooling2D\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "#@title Run this to import classifiers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "\n",
        "!rm -r /content/Data\n",
        "!git clone https://github.com/Neurotech-X-Columbia/Data.git\n",
        "!rm -r /content/Data/SSVEP/TF\n",
        "\n",
        "\n",
        "num_chans = 16\n",
        "fs = 125\n",
        "notch_f = 60\n",
        "Q_factor = 30\n",
        "lp_f = 2\n",
        "iir_hp_2 = signal.iirfilter(2, 2. / fs, btype='highpass')\n",
        "a, b = signal.iirnotch(notch_f, Q_factor, fs)\n",
        "\n",
        "def filtering(d):\n",
        "    filtered_sig = signal.filtfilt(iir_hp_2[0], iir_hp_2[1], d, padlen=0)\n",
        "    filtnnotch_sig = signal.filtfilt(a, b, filtered_sig, padlen=0)\n",
        "    return filtnnotch_sig\n",
        "\n",
        "\n",
        "datadir = r'/content/Data/SSVEP'\n",
        "category_folders = [x[0] for x in os.walk(datadir) if 'Bottom' in x[0] or 'Top' in x[0]]\n",
        "\n",
        "\n",
        "ids = ['TopLeft', 'TopRight', 'BottomLeft', 'BottomMiddle', 'BottomRight']\n",
        "positions = [[],[],[],[],[]] #each subarray corresponds to position above\n",
        "for i, pos in enumerate(ids):\n",
        "    for f in category_folders:\n",
        "        if f.endswith(pos):\n",
        "            for csv in os.listdir(f):\n",
        "                positions[i].append(np.genfromtxt(os.path.join(f, csv), delimiter=',')[1:])\n",
        "\n",
        "featnlab = []\n",
        "for ii in range(5):\n",
        "    for bruh, tri in enumerate(positions[ii]):\n",
        "        for chan in [14, 15]:\n",
        "            if not (np.isnan(tri).any()):\n",
        "                pow_fxx, fxx = psd(filtering(tri[chan, 125:]), Fs=fs) # what is the 125 here\n",
        "                featnlab.append(np.append(pow_fxx, ii))\n",
        "\n",
        "fnl = np.array(featnlab)\n",
        "np.random.shuffle(fnl)\n",
        "\n",
        "\n",
        "\n",
        "x = fnl[:, :129]\n",
        "augmented1 = np.random.normal(x)\n",
        "augmented2 = np.random.normal(x)\n",
        "x = np.concatenate((x, augmented1))\n",
        "x = np.concatenate((x, augmented2))\n",
        "\n",
        "y = fnl[:, 129:]\n",
        "y_2 = y\n",
        "y_3 = y\n",
        "y = np.concatenate((y, y_2))\n",
        "y = np.concatenate((y, y_3))\n",
        "\n",
        "\n",
        "\n",
        "y_cold = y \n",
        "#y_one_hot = pd.DataFrame(y). --> uncomment these lines to one-hot encode and run a deep-learning model\n",
        "#y_one_hot = pd.get_dummies(y_one_hot[0])\n",
        "#y = y_one_hot.to_numpy()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN model here\n",
        "m1 = KNeighborsClassifier(1).fit(x_train, y_train) # optimized\n",
        "preds = m1.predict(x_test)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twHGJf3-O559",
        "outputId": "6440762e-bbc9-42dc-97be-0b27513f1b8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8833333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ]
    }
  ]
}