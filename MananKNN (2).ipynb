{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k0w3lDbOj0D",
        "outputId": "08db2e72-b22c-4e7a-a483-2caeaa907082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data'...\n",
            "remote: Enumerating objects: 4365, done.\u001b[K\n",
            "remote: Counting objects: 100% (467/467), done.\u001b[K\n",
            "remote: Compressing objects: 100% (360/360), done.\u001b[K\n",
            "remote: Total 4365 (delta 112), reused 456 (delta 104), pack-reused 3898\u001b[K\n",
            "Receiving objects: 100% (4365/4365), 307.14 MiB | 18.08 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "Checking out files: 100% (3993/3993), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Initalize Script (may take a few minutes)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from matplotlib.mlab import psd\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import tabulate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "from pywt import wavedec\n",
        "from functools import reduce\n",
        "from scipy import signal\n",
        "from scipy.stats import entropy\n",
        "from scipy.fft import fft, ifft\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold,cross_validate\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "import matplotlib.pyplot as plt;\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D,Conv2D,Add\n",
        "from tensorflow.keras.layers import MaxPool1D, MaxPooling2D\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "#@title Run this to import classifiers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "\n",
        "!rm -r /content/Data\n",
        "!git clone https://github.com/Neurotech-X-Columbia/Data.git\n",
        "#!git clone -b MatheuBranch https://github.com/Neurotech-X-Columbia/Data.git\n",
        "#!rm -r /content/Data/SSVEP/GG\n",
        "#!rm -r /content/Data/SSVEP/JP\n",
        "#!rm -r /content/Data/SSVEP/GB\n",
        "#!rm -r /content/Data/SSVEP/MC\n",
        "!rm -r /content/Data/SSVEP/TF\n",
        "#!rm -r /content/Data/SSVEP/NG\n",
        "#!rm -r /content/Data/SSVEP/KK\n",
        "\n",
        "\n",
        "\n",
        "num_chans = 16\n",
        "fs = 125\n",
        "notch_f = 60\n",
        "Q_factor = 30\n",
        "lp_f = 2\n",
        "iir_hp_2 = signal.iirfilter(2, 2. / fs, btype='highpass')\n",
        "a, b = signal.iirnotch(notch_f, Q_factor, fs)\n",
        "\n",
        "def filtering(d):\n",
        "    filtered_sig = signal.filtfilt(iir_hp_2[0], iir_hp_2[1], d, padlen=0)\n",
        "    filtnnotch_sig = signal.filtfilt(a, b, filtered_sig, padlen=0)\n",
        "    return filtnnotch_sig\n",
        "\n",
        "\n",
        "datadir = r'/content/Data/SSVEP'\n",
        "category_folders = [x[0] for x in os.walk(datadir) if 'Bottom' in x[0] or 'Top' in x[0]]\n",
        "\n",
        "\n",
        "ids = ['TopLeft', 'TopRight', 'BottomLeft', 'BottomMiddle', 'BottomRight']\n",
        "positions = [[],[],[],[],[]] #each subarray corresponds to position above\n",
        "for i, pos in enumerate(ids):\n",
        "    for f in category_folders:\n",
        "        if f.endswith(pos):\n",
        "            for csv in os.listdir(f):\n",
        "                positions[i].append(np.genfromtxt(os.path.join(f, csv), delimiter=',')[1:])\n",
        "\n",
        "featnlab = []\n",
        "for ii in range(5):\n",
        "    for bruh, tri in enumerate(positions[ii]):\n",
        "        for chan in [14, 15]:\n",
        "            if not (np.isnan(tri).any()):\n",
        "                pow_fxx, fxx = psd(filtering(tri[chan, 125:]), Fs=fs) # what is the 125 here\n",
        "                featnlab.append(np.append(pow_fxx, ii))\n",
        "\n",
        "fnl = np.array(featnlab)\n",
        "np.random.shuffle(fnl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = fnl[:, :129]\n",
        "augmented1 = np.random.normal(x)\n",
        "augmented2 = np.random.normal(x)\n",
        "augmented3 = np.random.normal(x)\n",
        "augmented4 = np.random.normal(x)\n",
        "augmented5 = np.random.normal(x)\n",
        "augmented6 = np.random.normal(x)\n",
        "augmented7 = np.random.normal(x)\n",
        "x = np.concatenate((x, augmented1))\n",
        "x = np.concatenate((x, augmented2))\n",
        "x = np.concatenate((x, augmented3))\n",
        "x = np.concatenate((x, augmented4))\n",
        "x = np.concatenate((x, augmented5))\n",
        "x = np.concatenate((x, augmented6))\n",
        "x = np.concatenate((x, augmented7))\n",
        "\n",
        "y = fnl[:, 129:]\n",
        "y_1 = y\n",
        "y_2 = y\n",
        "y_3 = y\n",
        "y_4 = y\n",
        "y_5 = y\n",
        "y_6 = y\n",
        "y_7 = y\n",
        "y = np.concatenate((y, y_1))\n",
        "y = np.concatenate((y, y_2))\n",
        "y = np.concatenate((y, y_3))\n",
        "y = np.concatenate((y, y_4))\n",
        "y = np.concatenate((y, y_5))\n",
        "y = np.concatenate((y, y_6))\n",
        "y = np.concatenate((y, y_7))\n",
        "y = y.ravel()\n",
        "\n",
        "\n",
        "y_cold = y \n",
        "y_one_hot = pd.DataFrame(y)#. --> uncomment these lines to one-hot encode and run a deep-learning model\n",
        "y_one_hot = pd.get_dummies(y_one_hot[0])\n",
        "y = y_one_hot.to_numpy()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .05)"
      ],
      "metadata": {
        "id": "IN6B3IjjniOp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this cell to conduct hyper-paramter optimization for various classifiers\n",
        "results = []\n",
        "depth = range(10, 20)\n",
        "estimators = range(10, 20)\n",
        "max_f = range(10, 20)\n",
        "n = range(1, 10)\n",
        "n2 = range(1,20)\n",
        "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "for i in n2:\n",
        "  print(i)\n",
        "  model = KNeighborsClassifier(1, p = i).fit(x_train, y_train)\n",
        "  preds = model.predict(x_test)\n",
        "  acc = accuracy_score(y_test, preds)\n",
        "  results.append([i, acc])\n",
        "results = pd.DataFrame(results)\n",
        "#results = results.sort_values(by=[3])\n",
        "results\n",
        "\n"
      ],
      "metadata": {
        "id": "fxDJKoPAGkXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M2wIDFvZYxb",
        "outputId": "480f2900-eeb5-4196-a785-d0cd2ee25edf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 129)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN model here\n",
        "m1 = KNeighborsClassifier(1, p = 4).fit(x_train, y_train) # optimized\n",
        "preds = m1.predict(x_test)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twHGJf3-O559",
        "outputId": "2b4df762-1414-4210-8dab-907464a5e05d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(x_test.shape[1], 1))\n",
        "z = tf.keras.layers.Conv1D(25, kernel_size = 2, activation = 'relu')(inputs)\n",
        "z = tf.keras.layers.Conv1D(25, kernel_size = 1, activation = 'relu')(z)\n",
        "z = tf.keras.layers.Flatten()(z)\n",
        "z = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
        "z = tf.keras.layers.Dropout(0.05)(z)\n",
        "outputs = tf.keras.layers.Dense(5 , activation='softmax')(z)\n",
        "model_cnn = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcin0ip2OZKz",
        "outputId": "e62035f1-eea0-4fe5-b917-1240258e7b1e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 129, 1), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 129, 5), dtype=tf.float32, name=None), name='dense_15/Softmax:0', description=\"created by layer 'dense_15'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1000\n",
        "\n",
        "history = model_cnn.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=500,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "7K8i9Gn1OgHm",
        "outputId": "37dcdce4-904c-47c8-edcd-e90f2db2ad70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-951e749d00a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )]\n\u001b[1;32m     26\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1788, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 129, 5) are incompatible\n"
          ]
        }
      ]
    }
  ]
}